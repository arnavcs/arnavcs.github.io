<html lang="en">
 <head>
  <meta charset="utf-8">
  <title>4D Raymarching Pathtracer Devlog - Arnav Kumar</title>
  <link rel="stylesheet" href="../../assets/style.css">
 </head>
 <body>
  <header>
   <hgroup><a href="../../index.html">[Arnav Kumar]</a> / <h1>4D Raymarching Pathtracer Devlog</h1></hgroup>
  </header>
  <main>
   <h2>4D Camera and GIF output</h2>
   <hgroup>
    <h3>--- Day 01 ---</h3>
    <span class="right">Jul 09 2025</span>
   </hgroup>
   <p>
    Today I started working on the project.
    I started off by cleaning up my code for raytracing significantly and refactoring GIF rendering and removing all of the OpenGL window work.
    I also had to switch to C++17 for <code>std::variant</code>.
    I should have working GIF output now, but I don't have a way to test it without creating a scene, and a simple render.
    So for now, I am working on the simple raymarching.
   </p>
   <p>
    I have created structs for two objects, a hypersphere (3-sphere) and a hyperplane.
    I then define <code>Object</code> as a variant of the two types of objects, and define <code>Scene</code> simply as a vector of objects.
    I was orignally getting lots of errors when I tried implementing this with C tagged unions, but I realized that C++ has some different quirks than C, so I had to learn to use C++ variants.
   </p>
   <hgroup>
    <h3>--- Day 02 ---</h3>
    <span class="right">Jul 10 2025</span>
   </hgroup>
   <p>
    I changed the second object from a hyperplane to a halfspace for now.
    I'll probably add a hyperplane later, but for now, I think the simplicity of the halfspace is prefered.
    I have written up both of the SDFs for the two object types I have right now, using a <code>switch</code> on the <code>variant::index()</code>.
   </p>
   <p>
    I finished writing up the camera.
    The way I have set up the camera is that it contains information about its location and 4 orthonormal vectors which form its heading.
    Additionally, the function <code>nextCamera</code> updates the camera for the next frame of the GIF based on its current state, thus shifting the camera position.
   </p>
   <h2>Simple Raymarching</h2>
   <p>
    Today, I have also written a simple raymarcher that uses the SDFs to simply check if there is an intersection or not, and colour white if there is.
    Like this, I can create simple renders of the 3D hyperplane the camera is pointing in!
   </p>
   <p>
    I have 2 below, one which is a static render, and the second being a render with the camera moving in the 4th dimension.
    Both scenes are of two hyperspheres with a halfplane, where colouring is based on the atan of a component of the intersection position.
    This colouring method is currently hard coded and something that I'll change (it's not very pleasant to look at either).
   </p>
   <figure>
    <img src="./render-01.gif" width=50%>
    <figcaption>Figure 1: 100 marching steps per ray</figcaption>
   </figure>
   <p>
    Here you can clearly see that as we move into the orthogonal direction, the spheres seem to "get smaller".
    This is just because the sphere slice of the hypersphere we are observing is smaller in the camera's new hyperplane.
   </p>
   <p>
    As you can see, there is undesirable warping of the shape of the halfplane near the hypersphere and near the horizon.
    This is because the halfspace and rays ends up at grazing angles near the horizon, and near the edge of the sphere, rays have to spend many steps as well.
    Thus, both of these issues can be resolved with more marching steps, like in the render below (which is only one frame).
   </p>
   <figure>
    <img src="./render-02.gif" width=50%>
    <figcaption>Figure 2: 10000 marching steps per ray</figcaption>
   </figure>
   <hgroup>
    <h3>--- Day 03 ---</h3>
    <span class="right">Jul 11 2025</span>
   </hgroup>
   <p>
    Today, I started off with gradient approximation at the hit location.
    From knowing which object yielded the smallest SDF, we can approximate that object's SDF's gradient at the hit location to determine the normal direction.
    The normal is given by n defined as follows (i, j, k, l are the standard 4D unit vectors, f is the sdf of the object, and p is the hit position).
   </p>
   <code>
<pre>n = norm(m)

m.x = f(p + εi) - f(p - εi)
m.y = f(p + εj) - f(p - εj)
m.z = f(p + εk) - f(p - εk)
m.w = f(p + εl) - f(p - εl)</pre>
   </code>
   <p>
    From this, we can shade our hit location according to the normal position to get some sort of idea of if our normals our correct.
    Renders in figure 3 and 4 show a single hypersphere with normal shading.
   </p>
   <figure>
    <img src="./render-03.gif" width=50%>
    <figcaption>Figure 3: still camera; RGB = normal xyz</figcaption>
   </figure>
   <p>
    One interesting thing to note is that we almost form "cells" of colour in the first example.
    This probably relate to intersections where the w component of the normal is nearly 0, and is likely a floating point rounding issue.
    As you will see in the next render though, this is a negligible edge case, and the result is not pronounced.
   </p>
   <figure>
    <img src="./render-04.gif" width=50%>
    <figcaption>Figure 4: camera moves in w; RGB = normal xyw</figcaption>
   </figure>
   <p>
    In this render, we can see that as we move farther into slices of the hypersphere with larger w components, the overall blue in the sphere increases.
   </p>
   <p>
    Finally, if you are wondering (for example) why the reddest spot in Figure 3 is not the rightmost, it is because at the rightmost point of the sphere, the green and blue components will be half.
    We have to transform a range from [-1, 1] to [0, 1] for each of RGB, so it won't be pure red.
   </p>
   <hgroup>
    <h3>--- Day 04 ---</h3>
    <span class="right">Jul 12 2025</span>
   </hgroup>
   <p>
    Today, I just worked on creating a scene that I like.
    So far I only have the hypersphere and halfplane primitives, so I set up a scene with 2 hyperspheres in a room made of halfplanes.
    The hyperspheres have varying w components and radii.
   </p>
   <figure>
    <img src="./render-05.gif" width=50%>
    <figcaption>Figure 5: Room render</figcaption>
   </figure>
   <p>
    I had to increase the raymarching intersection epsilon to get the room to render properly without black edges.
   </p>
   <h2>Material Specification System</h2>
   <hgroup>
    <h3>--- Day 05 ---</h3>
    <span class="right">Jul 13 2025</span>
   </hgroup>
   <p>
    The implementation of the material system required me to redefine an object to not simply be a geometry, but to consist of both a geometry and a material.
    I have material as a <code>variant</code> of glass, mirror, and textured materials for now.
   </p>
   <p>
    This mirror material uses the normals to reflect the rays.
    As you will see though, what I thought was going to be a negligible problem (inaccurate normals) will become very noticable.
   </p>
   <figure>
    <img src="./render-06.gif" width=50%>
    <figcaption>Figure 6: Mirror material</figcaption>
   </figure>
   <p>
    I definitely need a way to fix this.
    Also, I had some issues with rays getting infinitely reflected on mirror surfaces because it is infinitely registered as reflection.
    I fixed that temporarily by shifting the position of the reflected ray has to start at an offset away from the sphere.
   </p>
   <p>
    One interesting code architecture choice is that I am letting the texture be a <code>std::functional&lt;float4, float4&gt;</code>.
    Thus, the texture can be specified as any function of position and surface normal.
   </p>
   <hgroup>
    <h3>--- Day 06 ---</h3>
    <span class="right">Jul 14 2025</span>
   </hgroup>
   <p>
    I started off today with reflecting on what I did yesterday, and I realized that yesterday I spent a lot of time trying to fix avoiding a ray getting infinitely reflected.
    Really, all I needed was a check on the value of <code>dot(n, d)</code> where <code>d</code> is the ray direction.
    If this value is negative, then we reflect, otherwise, we just let it march.
    And so with that, I tried to render the sphere scene to no avail, just black spheres.
    But increasing the marching intersection epsilon and increasing the max number of steps for a ray yielded an image.
   </p>
   <figure>
    <img src="./render-07.gif" width=50%>
    <figcaption>Figure 7: Improved ray reflection</figcaption>
   </figure>
   <p>
    Surprisingly, the wonky steppyness that I was getting earlier is also gone...
    The render time is extremely high, as we may expect, though, so I think that I will take an aside to work on getting parallel computing working.
   </p>
   <hgroup>
    <h3>--- Day 07 ---</h3>
    <span class="right">Jul 15 2025</span>
   </hgroup>
   <p>
    Starting off with execution time speedups, I started with working on implementing some sort of concurrency or parallelism.
    I decided to use <a href="https://www.openmp.org/">(OpenMP)</a>, because it seems to be rather straightforward.
    Here is the time takes to render the scene in figure 7 with and without parallelism.
   </p>
   <p>
    Without parallelism
   </p>
   <code>
<pre>real	0m3.180s
user	0m3.153s
sys	0m0.017s</pre>
   </code>
   <p>
    With parallelism
   </p>
   <code>
<pre>real	0m0.142s
user	0m6.107s
sys	0m0.078s</pre>
   </code>
   <p>
    As you can see, the speedup is substantial.
   </p>
   <p>
    And with that speedup, here is a more flushed out render of the two mirror hyperspheres scene.
   </p>
   <figure>
    <img src="./render-08.gif" width=50%>
    <figcaption>Figure 8: More impressive render</figcaption>
   </figure>
   <hgroup>
    <h3>--- Day 08 ---</h3>
    <span class="right">Jul 16 2025</span>
   </hgroup>
   <p>
    Today I refactored the material system with one objective in mind: to allow for one object to have different materials based on where on the material you are.
    To accomplish this, each object has a geometry, and a material map.
    The material map is a function which returns a material based on position and normal.
   </p>
   <p>
    And with this new material system, I wanted to show that rays which are reflected can go into a different subspace.
    To this end, I setup the material map as follows for the walls:
   </p>
   <code>
<pre>MaterialMap walls = [](float4 p, float4 n) {
    float4 nn = 0.5f * n + float4(0.5f);
    return Light{(nn.x + 0.5f * nn.z) * RED + (nn.y + 0.5f * nn.z) * GREEN + p.w * BLUE};
};</pre>
   </code>
   <p>
    And with this material, if we render one frame with the w component as 0, we get the following:
   </p>
   <figure>
    <img src="./render-09.gif" width=50%>
    <figcaption>Figure 9: Reflected rays exit camera's rendering hyperplane</figcaption>
   </figure>
   <p>
    From here, we can evidently see that the rays reflecting from the small hypersphere get bounced out of the camera's hyperplane.
    This is because the center of the smaller hypersphere is not in the camera's hyperplane (it has a different w component).
   </p>
   <h2>Fresnel Refraction</h2>
   <hgroup>
    <h3>--- Day 09 ---</h3>
    <span class="right">Jul 22 2025</span>
   </hgroup>
   <p>
    I started off today with a refactor of the raymarching algorithm by having a queue of <code>RayInfo</code> rays which contain the ray we are currently tracing, as well as the current accumulated colour and "tint" associated with the ray.
    The reason that I include this is because I would like the Fresnel refraction to be exact and to accomplish this, I need to have the rays split into 2.
    While I could optionally keep this information in the start and use explicit recursion, this would not be good as I keep a very high ray marching recursion depth, and I would be limited by the stack recursion depth.
   </p>
   <p>
    Also, it is good to note that I had to change the closest intersection to use the absolute value of the SDF since we'll be marching inside an object.
    In order to calculate the refracted ray we also need to know both the index of refraction before and after refraction.
    To determine this, I add a condition that the objects in the space cannot overlap, and it they do, raymarching behaviour at overlapping spaces is undefined.
   </p>
   <p>
    I then spent the rest of the day refactoring all of the code to allow for sufficient information when calculating the resulting rays from an intersection.
   </p>
   <hgroup>
    <h3>--- Day 10 ---</h3>
    <span class="right">Jul 23 2025</span>
   </hgroup>
   <p>
    Today, I finished the refactoring. I added a material for air, and also changed the way I match over variants to avoid using indices and start using <code>std::holds_alternative</code>.
    I implemented refraction according to <a href="https://en.wikipedia.org/wiki/Snell%27s_law#Vector_form">(this formula)</a> on the vector form section of the Snell's Law wikipedia.
    I seem to be having some issues though, because my render is ending up black like the following.
   </p>
   <figure>
    <img src="./render-10.gif" width=50%>
    <figcaption>Figure 10: Issue with refraction code</figcaption>
   </figure>
   <p>
    So, after some time, I figured out what the issue is. I seem to always register a hit once I hit a glass surface regardless of where we are in relation to the glass surface.
    This turned out to be because I scoped certain variables incorrectly and they didn't reset in value like they should have.
    So far though, I just have either total internal reflection or refraction, and no Fresnel refraction.
    Here is an example render though, where the big hypersphere is a glass sphere and the smaller one on the right is a mirror.
   </p>
   <figure>
    <img src="./render-11.gif" width=50%>
    <figcaption>Figure 11: Non-Fresnel refraction</figcaption>
   </figure>
   <p>
    With this, there are two interesting things to notice.
   </p>
   <ol>
    <li>The w component of the position corresponds to the blue channel of the wall colour</li>
    <li>I added two walls with normals in the w direction which are visible as mossy green and lavender in the glass and mirror</li>
   </ol>
   <hgroup>
    <h3>--- Day 11 ---</h3>
    <span class="right">Jul 24 2025</span>
   </hgroup>
   <p>
    In accordance to <a href="https://en.wikipedia.org/wiki/Schlick%27s_approximation">(Schlick's approximation)</a>, I implemented Fresnel refraction.
    There was a hiccup relating to having a queue which grew indefinitely because I never seemed to have a termination statement that was met.
    This was due to an error in implementing the reflectance coefficient, in which the sign I assumed for the vectors <code>l</code> and <code>n</code> was incorrect.
    I referenced the "Background: Physics and Math of Shading (Naty Hoffman)" course notes for a <a href="https://blog.selfshadow.com/publications/s2012-shading-course/#course_content">(SIGGRAPH 2012 course)</a> for more information about the Schlick approximation.
   </p>
   <figure>
    <img src="./render-12.gif" width=50%>
    <figcaption>Figure 12: Fresnel refraction with refractive index 2</figcaption>
   </figure>
   <figure>
    <img src="./render-13.gif" width=50%>
    <figcaption>Figure 13: Fresnel refraction with refractive index 1.4</figcaption>
   </figure>
   <h2>SDF Manipulations</h2>
   <p>
    The purpose of this section is to to the following:
   </p>
   <ul>
    <li>introduce hypercube geometry</li>
    <li>geometry infinite tiling</li>
    <li>geometry rounding</li>
    <li>geometry onioning</li>
    <li>modify SDFs for translations and rotations</li>
    <li>remodel existing geometry to use transformations instead of having position parameters</li>
   </ul>
   <hgroup>
    <h3>--- Day 12 ---</h3>
    <span class="right">Jul 25 2025</span>
   </hgroup>
   <p>
    Today, I have introduced hyperbox geometry, and added SDFs for translations and rotations.
    I have additionally remodelled the existing hypersphere and halfplane code to be described by simply radius and normal vectors, and to leave the position and offset as results of translation.
    Here is an example of a hypercube that is rotated into an interesting position.
   </p>
   <figure>
    <img src="./render-14.gif" width=50%>
    <figcaption>Figure 14: Hypercube at an orientation</figcaption>
   </figure>
   <hgroup>
    <h3>--- Day 13 ---</h3>
    <span class="right">Jul 26 2025</span>
   </hgroup>
   <p>
    I finished off the other SDF manipulations like onioning and rounding.
    In fact, here is a onioned, rounded hyperbox with Fresnel refraction.
   </p>
   <figure>
    <img src="./render-15.gif" width=50%>
    <figcaption>Figure 15: Onioned, rounded, rotated glass hyperbox</figcaption>
   </figure>
   <p>
    There is a lot of noise in this render.
    This is something that we may expect because some paths are terminated very early compared to how long they should proceed.
    This means we potentially only get less contributing paths.
    When volumetric fog and volumetric rendering is added, this should be remediated somewhat.
   </p>
   <h2>BDRFs for Monte Carlo and Pathtracing with Parallel Computation</h2>
   <h2>Volumetric Fog</h2>
   <p>
    In order to add volumetric fog, I have to consider two things: the absorption of light by the fog, and the scattering of the light from the fog.
    Implementing the absorption of light seems very doable since I can simply calculate the absorption based on distance the ray marches.
   </p>
   <h2>Denoising</h2>
  </main>
  <footer>
   <p>
    Contributing to a lightweight and accessible web.
   </p>
  </footer>
 </body>
</html>
