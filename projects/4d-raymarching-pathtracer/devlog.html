<html lang="en">
 <head>
  <meta charset="utf-8">
  <title>4D Raymarching Pathtracer Devlog - Arnav Kumar</title>
  <link rel="stylesheet" href="../../assets/style.css">
 </head>
 <body>
  <header>
   <hgroup><a href="../../index.html">[Arnav Kumar]</a> / <h1>4D Raymarching Pathtracer Devlog</h1></hgroup>
  </header>
  <main>
   <h2>4D Camera and GIF output</h2>
   <hgroup>
    <h3>--- Day 01 ---</h3>
    <span class="right">Jul 09 2025</span>
   </hgroup>
   <p>
    Today I started working on the project.
    I started off by cleaning up my code for raytracing significantly and refactoring GIF rendering and removing all of the OpenGL window work.
    I also had to switch to C++17 for <code>std::variant</code>.
    I should have working GIF output now, but I don't have a way to test it without creating a scene, and a simple render.
    So for now, I am working on the simple raymarching.
   </p>
   <p>
    I have created structs for two objects, a hypersphere (3-sphere) and a hyperplane.
    I then define <code>Object</code> as a variant of the two types of objects, and define <code>Scene</code> simply as a vector of objects.
    I was orignally getting lots of errors when I tried implementing this with C tagged unions, but I realized that C++ has some different quirks than C, so I had to learn to use C++ variants.
   </p>
   <hgroup>
    <h3>--- Day 02 ---</h3>
    <span class="right">Jul 10 2025</span>
   </hgroup>
   <p>
    I changed the second object from a hyperplane to a halfspace for now.
    I'll probably add a hyperplane later, but for now, I think the simplicity of the halfspace is prefered.
    I have written up both of the SDFs for the two object types I have right now, using a <code>switch</code> on the <code>variant::index()</code>.
   </p>
   <p>
    I finished writing up the camera.
    The way I have set up the camera is that it contains information about its location and 4 orthonormal vectors which form its heading.
    Additionally, the function <code>nextCamera</code> updates the camera for the next frame of the GIF based on its current state, thus shifting the camera position.
   </p>
   <h2>Simple Raymarching</h2>
   <p>
    Today, I have also written a simple raymarcher that uses the SDFs to simply check if there is an intersection or not, and colour white if there is.
    Like this, I can create simple renders of the 3D hyperplane the camera is pointing in!
   </p>
   <p>
    I have 2 below, one which is a static render, and the second being a render with the camera moving in the 4th dimension.
    Both scenes are of two hyperspheres with a halfplane, where colouring is based on the atan of a component of the intersection position.
    This colouring method is currently hard coded and something that I'll change (it's not very pleasant to look at either).
   </p>
   <figure>
    <img src="./render-01.gif" width=50%>
    <figcaption>Figure 1: 100 marching steps per ray</figcaption>
   </figure>
   <p>
    Here you can clearly see that as we move into the orthogonal direction, the spheres seem to "get smaller".
    This is just because the sphere slice of the hypersphere we are observing is smaller in the camera's new hyperplane.
   </p>
   <p>
    As you can see, there is undesirable warping of the shape of the halfplane near the hypersphere and near the horizon.
    This is because the halfspace and rays ends up at grazing angles near the horizon, and near the edge of the sphere, rays have to spend many steps as well.
    Thus, both of these issues can be resolved with more marching steps, like in the render below (which is only one frame).
   </p>
   <figure>
    <img src="./render-02.gif" width=50%>
    <figcaption>Figure 2: 10000 marching steps per ray</figcaption>
   </figure>
   <hgroup>
    <h3>--- Day 03 ---</h3>
    <span class="right">Jul 11 2025</span>
   </hgroup>
   <p>
    Today, I started off with gradient approximation at the hit location.
    From knowing which object yielded the smallest SDF, we can approximate that object's SDF's gradient at the hit location to determine the normal direction.
    The normal is given by n defined as follows (i, j, k, l are the standard 4D unit vectors, f is the sdf of the object, and p is the hit position).
   </p>
   <code>
<pre>n = norm(m)

m.x = f(p + εi) - f(p - εi)
m.y = f(p + εj) - f(p - εj)
m.z = f(p + εk) - f(p - εk)
m.w = f(p + εl) - f(p - εl)</pre>
   </code>
   <p>
    From this, we can shade our hit location according to the normal position to get some sort of idea of if our normals our correct.
    Renders in figure 3 and 4 show a single hypersphere with normal shading.
   </p>
   <figure>
    <img src="./render-03.gif" width=50%>
    <figcaption>Figure 3: still camera; RGB = normal xyz</figcaption>
   </figure>
   <p>
    One interesting thing to note is that we almost form "cells" of colour in the first example.
    This probably relate to intersections where the w component of the normal is nearly 0, and is likely a floating point rounding issue.
    As you will see in the next render though, this is a negligible edge case, and the result is not pronounced.
   </p>
   <figure>
    <img src="./render-04.gif" width=50%>
    <figcaption>Figure 4: camera moves in w; RGB = normal xyw</figcaption>
   </figure>
   <p>
    In this render, we can see that as we move farther into slices of the hypersphere with larger w components, the overall blue in the sphere increases.
   </p>
   <p>
    Finally, if you are wondering (for example) why the reddest spot in Figure 3 is not the rightmost, it is because at the rightmost point of the sphere, the green and blue components will be half.
    We have to transform a range from [-1, 1] to [0, 1] for each of RGB, so it won't be pure red.
   </p>
   <hgroup>
    <h3>--- Day 04 ---</h3>
    <span class="right">Jul 12 2025</span>
   </hgroup>
   <p>
    Today, I just worked on creating a scene that I like.
    So far I only have the hypersphere and halfplane primitives, so I set up a scene with 2 hyperspheres in a room made of halfplanes.
    The hyperspheres have varying w components and radii.
   </p>
   <figure>
    <img src="./render-05.gif" width=50%>
    <figcaption>Figure 5: Room render</figcaption>
   </figure>
   <p>
    I had to increase the raymarching intersection epsilon to get the room to render properly without black edges.
   </p>
   <h2>Material Specification System</h2>
   <hgroup>
    <h3>--- Day 05 ---</h3>
    <span class="right">Jul 13 2025</span>
   </hgroup>
   <p>
    The implementation of the material system required me to redefine an object to not simply be a geometry, but to consist of both a geometry and a material.
    I have material as a <code>variant</code> of glass, mirror, and textured materials for now.
   </p>
   <p>
    This mirror material uses the normals to reflect the rays.
    As you will see though, what I thought was going to be a negligible problem (inaccurate normals) will become very noticable.
   </p>
   <figure>
    <img src="./render-06.gif" width=50%>
    <figcaption>Figure 6: Mirror material</figcaption>
   </figure>
   <p>
    I definitely need a way to fix this.
    Also, I had some issues with rays getting infinitely reflected on mirror surfaces because it is infinitely registered as reflection.
    I fixed that temporarily by shifting the position of the reflected ray has to start at an offset away from the sphere.
   </p>
   <p>
    One interesting code architecture choice is that I am letting the texture be a <code>std::functional&lt;float4, float4&gt;</code>.
    Thus, the texture can be specified as any function of position and surface normal.
   </p>
   <hgroup>
    <h3>--- Day 06 ---</h3>
    <span class="right">Jul 14 2025</span>
   </hgroup>
   <p>
    I started off today with reflecting on what I did yesterday, and I realized that yesterday I spent a lot of time trying to fix avoiding a ray getting infinitely reflected.
    Really, all I needed was a check on the value of <code>dot(n, d)</code> where <code>d</code> is the ray direction.
    If this value is negative, then we reflect, otherwise, we just let it march.
    And so with that, I tried to render the sphere scene to no avail, just black spheres.
    But increasing the marching intersection epsilon and increasing the max number of steps for a ray yielded an image.
   </p>
   <figure>
    <img src="./render-07.gif" width=50%>
    <figcaption>Figure 7: Improved ray reflection</figcaption>
   </figure>
   <p>
    Surprisingly, the wonky steppyness that I was getting earlier is also gone...
    The render time is extremely high, as we may expect, though, so I think that I will take an aside to work on getting parallel computing working.
   </p>
   <hgroup>
    <h3>--- Day 07 ---</h3>
    <span class="right">Jul 15 2025</span>
   </hgroup>
   <p>
    Starting off with execution time speedups, I started with working on implementing some sort of concurrency or parallelism.
    I decided to use <a href="https://www.openmp.org/">(OpenMP)</a>, because it seems to be rather straightforward.
    Here is the time takes to render the scene in figure 7 with and without parallelism.
   </p>
   <p>
    Without parallelism
   </p>
   <code>
<pre>real	0m3.180s
user	0m3.153s
sys	0m0.017s</pre>
   </code>
   <p>
    With parallelism
   </p>
   <code>
<pre>real	0m0.142s
user	0m6.107s
sys	0m0.078s</pre>
   </code>
   <p>
    As you can see, the speedup is substantial.
   </p>
   <p>
    And with that speedup, here is a more flushed out render of the two mirror hyperspheres scene.
   </p>
   <figure>
    <img src="./render-08.gif" width=50%>
    <figcaption>Figure 8: More impressive render</figcaption>
   </figure>
   <hgroup>
    <h3>--- Day 08 ---</h3>
    <span class="right">Jul 16 2025</span>
   </hgroup>
   <p>
    Today I refactored the material system with one objective in mind: to allow for one object to have different materials based on where on the material you are.
    To accomplish this, each object has a geometry, and a material map.
    The material map is a function which returns a material based on position and normal.
   </p>
   <p>
    And with this new material system, I wanted to show that rays which are reflected can go into a different subspace.
    To this end, I setup the material map as follows for the walls:
   </p>
   <code>
<pre>MaterialMap walls = [](float4 p, float4 n) {
    float4 nn = 0.5f * n + float4(0.5f);
    return Light{(nn.x + 0.5f * nn.z) * RED + (nn.y + 0.5f * nn.z) * GREEN + p.w * BLUE};
};</pre>
   </code>
   <p>
    And with this material, if we render one frame with the w component as 0, we get the following:
   </p>
   <figure>
    <img src="./render-09.gif" width=50%>
    <figcaption>Figure 9: Reflected rays exit camera's rendering hyperplane</figcaption>
   </figure>
   <p>
    From here, we can evidently see that the rays reflecting from the small hypersphere get bounced out of the camera's hyperplane.
    This is because the center of the smaller hypersphere is not in the camera's hyperplane (it has a different w component).
   </p>
   <h2>Fresnel Refraction</h2>
   <hgroup>
    <h3>--- Day 09 ---</h3>
    <span class="right">Jul 22 2025</span>
   </hgroup>
   <p>
    I started off today with a refactor of the raymarching algorithm by having a queue of <code>RayInfo</code> rays which contain the ray we are currently tracing, as well as the current accumulated colour and "tint" associated with the ray.
    The reason that I include this is because I would like the Fresnel refraction to be exact and to accomplish this, I need to have the rays split into 2.
    While I could optionally keep this information in the start and use explicit recursion, this would not be good as I keep a very high ray marching recursion depth, and I would be limited by the stack recursion depth.
   </p>
   <p>
    Also, it is good to note that I had to change the closest intersection to use the absolute value of the SDF since we'll be marching inside an object.
    In order to calculate the refracted ray we also need to know both the index of refraction before and after refraction.
    To determine this, I add a condition that the objects in the space cannot overlap, and it they do, raymarching behaviour at overlapping spaces is undefined.
   </p>
   <p>
    I then spent the rest of the day refactoring all of the code to allow for sufficient information when calculating the resulting rays from an intersection.
   </p>
   <hgroup>
    <h3>--- Day 10 ---</h3>
    <span class="right">Jul 23 2025</span>
   </hgroup>
   <p>
    Today, I finished the refactoring. I added a material for air, and also changed the way I match over variants to avoid using indices and start using <code>std::holds_alternative</code>.
    I implemented refraction according to <a href="https://en.wikipedia.org/wiki/Snell%27s_law#Vector_form">(this formula)</a> on the vector form section of the Snell's Law wikipedia.
    I seem to be having some issues though, because my render is ending up black like the following.
   </p>
   <figure>
    <img src="./render-10.gif" width=50%>
    <figcaption>Figure 10: Issue with refraction code</figcaption>
   </figure>
   <p>
    So, after some time, I figured out what the issue is. I seem to always register a hit once I hit a glass surface regardless of where we are in relation to the glass surface.
    This turned out to be because I scoped certain variables incorrectly and they didn't reset in value like they should have.
    So far though, I just have either total internal reflection or refraction, and no Fresnel refraction.
    Here is an example render though, where the big hypersphere is a glass sphere and the smaller one on the right is a mirror.
   </p>
   <figure>
    <img src="./render-11.gif" width=50%>
    <figcaption>Figure 11: Non-Fresnel refraction</figcaption>
   </figure>
   <p>
    With this, there are two interesting things to notice.
   </p>
   <ol>
    <li>The w component of the position corresponds to the blue channel of the wall colour</li>
    <li>I added two walls with normals in the w direction which are visible as mossy green and lavender in the glass and mirror</li>
   </ol>
   <hgroup>
    <h3>--- Day 11 ---</h3>
    <span class="right">Jul 24 2025</span>
   </hgroup>
   <p>
    In accordance to <a href="https://en.wikipedia.org/wiki/Schlick%27s_approximation">(Schlick's approximation)</a>, I implemented Fresnel refraction.
    There was a hiccup relating to having a queue which grew indefinitely because I never seemed to have a termination statement that was met.
    This was due to an error in implementing the reflectance coefficient, in which the sign I assumed for the vectors <code>l</code> and <code>n</code> was incorrect.
    I referenced the "Background: Physics and Math of Shading (Naty Hoffman)" course notes for a <a href="https://blog.selfshadow.com/publications/s2012-shading-course/#course_content">(SIGGRAPH 2012 course)</a> for more information about the Schlick approximation.
   </p>
   <figure>
    <img src="./render-12.gif" width=50%>
    <figcaption>Figure 12: Fresnel refraction with refractive index 2</figcaption>
   </figure>
   <figure>
    <img src="./render-13.gif" width=50%>
    <figcaption>Figure 13: Fresnel refraction with refractive index 1.4</figcaption>
   </figure>
   <h2>SDF Manipulations</h2>
   <p>
    The purpose of this section is to to the following:
   </p>
   <ul>
    <li>introduce hypercube geometry</li>
    <li>geometry infinite tiling</li>
    <li>geometry rounding</li>
    <li>geometry onioning</li>
    <li>modify SDFs for translations and rotations</li>
    <li>remodel existing geometry to use transformations instead of having position parameters</li>
   </ul>
   <hgroup>
    <h3>--- Day 12 ---</h3>
    <span class="right">Jul 25 2025</span>
   </hgroup>
   <p>
    Today, I have introduced hyperbox geometry, and added SDFs for translations and rotations.
    I have additionally remodelled the existing hypersphere and halfplane code to be described by simply radius and normal vectors, and to leave the position and offset as results of translation.
    Here is an example of a hypercube that is rotated into an interesting position.
   </p>
   <figure>
    <img src="./render-14.gif" width=50%>
    <figcaption>Figure 14: Hypercube at an orientation</figcaption>
   </figure>
   <hgroup>
    <h3>--- Day 13 ---</h3>
    <span class="right">Jul 26 2025</span>
   </hgroup>
   <p>
    I finished off the other SDF manipulations like onioning and rounding.
    In fact, here is a onioned, rounded hyperbox with Fresnel refraction.
   </p>
   <figure>
    <img src="./render-15.gif" width=50%>
    <figcaption>Figure 15: Onioned, rounded, rotated glass hyperbox</figcaption>
   </figure>
   <p>
    There is a lot of noise in this render.
    This is something that we may expect because some paths are terminated very early compared to how long they should proceed.
    This means we potentially only get less contributing paths.
    When volumetric fog and volumetric rendering is added, this should be remediated somewhat.
   </p>
   <p>
    The code for describing this scene is as follows:
   </p>
   <code>
<pre>MaterialMap wall = [](float4 p, float4 n) {
    float4 nn = 0.5f * n + float4(0.5f);
    return Light{(nn.x + 0.5f * nn.z) * RED
               + (nn.y + 0.5f * nn.z) * GREEN
               + (0.25f * p.w + 0.5f) * BLUE};
};

MaterialMap glass = [](float4 p, float4 n) {
    return Glass{Colour(0.7f), 1.4f};
};

Geometry room   = Hyperbox{float4(2.0f, 1.5f, 4.0f, 2.0f)};
Geometry roomT  = Translation{float4(0.0f, 0.0f, -2.0f, 0.0f), &room};
Geometry roomTO = Onion{0.1f, &roomT};

Geometry box    = Hyperbox{float4(0.5f)};
Geometry boxR   = Rotation{inverse(rotateAlong(0, 3, 20,
                                   rotateAlong(1, 3, 45,
                                   rotateAlong(1, 2, 30,
                                   rotateAlong(0, 2, 30, linalg::identity))))), &box};
Geometry boxRR  = Round{0.3f, &boxR};
Geometry boxRRO = Onion{0.2f, &boxRR};

scene.objects.push_back(Object{roomTO, wall});
scene.objects.push_back(Object{boxRRO, glass});</pre>
   </code>
   <p>
    I didn't mention this yesterday, but the way that I do rotations is by specifying the inverse rotation matrix.
    In specific, the inverse rotation matrix should by an isometry and have determinant 1.
    Since it's hard to come up with such rotation matrices, I decided to be able to construct such a matrix via the ability to rotate along a certain plane at a time.
    Namely, the function <code>rotateAlong</code> will replace the columns of the rotation matrix at specified indices with two new vectors which are linear combinations of the columns.
    Here is the actual implementation
   </p>
   <code>
<pre>float4x4 rotateAlong(int index1, int index2, float angleDegrees, float4x4 rotation) {
    float angleRadians = angleDegrees * DEG_TO_RAD;
    float4 n1 =  float(cos(angleRadians)) * rotation[index1]
               + float(sin(angleRadians)) * rotation[index2];
    float4 n2 = -float(sin(angleRadians)) * rotation[index1]
               + float(cos(angleRadians)) * rotation[index2];
    rotation[index1] = n1;
    rotation[index2] = n2;
    return rotation;
}</pre>
   </code>
   <hgroup>
    <h3>--- Day 14 ---</h3>
    <span class="right">Jul 28 2025</span>
   </hgroup>
   <p>
    Today I finished the infinite tiling code.
    I decided to implement tiling generally by allowing the user to specify a function which given a point in space, returns the center of the closest tiling cell.
    Then, I can denote the D4 tiling scheme as follows:
   </p>
   <code>
<pre>TilingCellMap d4Tiling = [](float4 p) {
    float4 tilingCell;

    int lx = std::ceil(p.x - 1.0f), rx = std::floor(p.x + 1.0f);
    int ly = std::ceil(p.y - 1.0f), ry = std::floor(p.y + 1.0f);
    int lz = std::ceil(p.z - 1.0f), rz = std::floor(p.z + 1.0f);
    int lw = std::ceil(p.w - 1.0f), rw = std::floor(p.w + 1.0f);
    lz = std::max(lz, 0); rz = std::max(rz, 0); // cut off points with negative z

    float distance2 = std::numeric_limits&lt;float&gt;::max();
    for (int x = lx; x <= rx; x++) {
    for (int y = ly; y <= ry; y++) {
    for (int z = lz; z <= rz; z++) {
    for (int w = lw; w <= rw; w++) {
        if ((x + y + z + w) % 2 != 0) continue;
        float4 candidate = float4(x, y, z, w);
        if (distance2 < length2(candidate - p)) continue;
        distance2 = length2(candidate - p);
        tilingCell = candidate;
    }}}}
    return tilingCell;
};</pre>
   </code>
   <p>
    Note that the way to exclude points with negative z component is to change <code>lz</code> and <code>rz</code>.
    I was originally first clamping the z component of <code>p</code> before performing any operations, but this is incorrect since the SDF function will think it's closest to a hypersphere with z value 1 instead of a closer sphere with z value 0 sometimes.
    This is the type of artifact that I was getting:
   </p>
   <figure>
    <img src="./render-16.gif" width=50%>
    <figcaption>Figure 16: Incorrect D4 lattice with hyperspheres</figcaption>
   </figure>
   <p>
    So I fixed this, and used hyperspheres of radius <code>1 / sqrt(2)</code> with the following shading scheme:
   </p>
   <code>
<pre>MaterialMap d4Shading = [d4Tiling](float4 p, float4 n) {
    std::vector&lt;Colour&gt; colours = {RED, GREEN, BLUE, YELLOW, TEAL, PINK, WHITE};
    int4 ip = apply([](float q) {return int(q);}, d4Tiling(p));
    int index = (2 * ip.x + 3 * ip.y + 5 * ip.z + 7 * ip.w) % colours.size();
    if (index < 0) index += colours.size();

    return Light{colours[index] * (0.5f + 0.5f * dot(n, normalize(float4(-1.0f))))};
};</pre>
   </code>
   <p>
    This yields the following visualization of the D4 packing of hyperspheres, the densest packing of hyperspheres in 4 dimensional real space.
   </p>
   <figure>
    <img src="./render-17.gif" width=50%>
    <figcaption>Figure 17: Densest equal hypersphere packing in 4D; w = [-1, 1]</figcaption>
   </figure>
   <p>
    You can see in some intermediate frames that there are an infinite number of these spheres.
    This is quite cool to observe since we have only one SDF for this whole object and only one shading rule.
    This is why I opted to let each object have a material map rather than a fixed material.
    In fact, since we defined our objects to have a material map, we can even make some of the spheres mirrors, like so:
   </p>
   <code>
<pre>MaterialMap d4Shading = [d4Tiling](float4 p, float4 n) {
    std::vector&lt;Colour&gt; colours = {RED, GREEN, BLUE, YELLOW, TEAL, PINK, WHITE};
    int4 ip = apply([](float q) {return int(q);}, d4Tiling(p));
    int index = (2 * ip.x + 3 * ip.y + 5 * ip.z + 7 * ip.w) % (colours.size() + 1);
    if (index < 0) index += (colours.size() + 1);

    Material m;
    if (index == (int) colours.size()) m = Mirror{0.7f * WHITE};
    else m = Light{colours[index] * (0.5f + 0.5f * dot(n, normalize(float4(-1.0f))))};
    return m;
};</pre>
   </code>
   <p>
    Which yields the following render:
   </p>
   <figure>
    <img src="./render-18.gif" width=50%>
    <figcaption>Figure 18: One object (with one SDF and one material map) renders as multiple distinct hyperspheres</figcaption>
   </figure>
   <h2>Pathtracing with Parallel Computation</h2>
   <hgroup>
    <h3>--- Day 15 ---</h3>
    <span class="right">Jul 30 2025</span>
   </hgroup>
   <p>
    The start for this is to first introduce tone mapping to shift from a space described by radiance and luminance to one which is described by pixel colour value.
    For now, I have opted to use Reinhard tone mapping with a slight modification.
    If <code>V</code> is the luminance of the pixel before mapping, then the luminance after tone mapping will be <code>Ar / (Ar + 1)</code> where <code>A</code> is some constant value.
    This is slightly more general than the typical tone mapping while still being really bare-bones.
    Higher values of <code>A</code> will result in overexposure, but lower values will result in images too dark.
    I've found <code>A = 2.0f</code> seems to be an acceptable middle ground.
   </p>
   <p>
    The second thing that I've done is to write a <code>randomPixelRay</code> function which randomly and uniformly samples an eye ray passing through the passed pixel.
    Along with this, I've introduced sampling for the render by averaging the resulting radiance for the rays for each pixel.
   </p>
   <figure>
    <img src="./render-19.gif" width=50%>
    <figcaption>Figure 19: Tone mapping and multiple eye rays</figcaption>
   </figure>
   <p>
    Recall that the parallel computation aspect is something that I've already implemented earlier.
   </p>
   <h2>Revisiting Fresnel and other BRDFs</h2>
   <hgroup>
    <h3>--- Day 16 ---</h3>
    <span class="right">Jul 31 2025</span>
   </hgroup>
   <p>
    I already have support for the BRDF of mirror surfaces and black body light sources.
    I am now looking at supporting the BRDFs of the following materials:
   </p>
   <ul>
    <li>Glass via Fresnel refraction</li>
    <li>Lambertian surfaces (ideal diffuse)</li>
    <li>Glossy materials via microfacets</li>
   </ul>
   <p>
    The implementation of this for glass is quite easy since I already ahve the diverging paths version implemented.
    All I have to do is choose to either refract into the material or reflect with the probability given by the computed coefficient.
    It returns an image like the following:
   </p>
   <figure>
    <img src="./render-20.gif" width=50%>
    <figcaption>Figure 20: Pathtracing Fresnel refraction with 5 samples / pixel</figcaption>
   </figure>
   <p>
    Now working on the ideal diffuse (Lambertian) BRDF, it is interesting to notice that I had to write a ton of new functions.
    Along with a variety of random sampling utilities, which make use of the C++ <code>&lt;random&gt;</code> header, I also had to write a householder reflection function.
   </p>
   <p>
    If you don't know, the householder reflection matrix <code>F</code> for a vector <code>x</code> satisfies <code>F * x = length(x) * e1</code> where <code>e1</code> is the first standard basis of the space.
    Why do I need such a function?
    Well, when a ray strikes a surface, suppose we want to uniformly sample the ball in the 3d space orthogonal to the surface normal.
    What is this subspace?
    If we have an orthonormal set of basis vectors, with one such vector being the surface normal, then we can instantly transform the standard unit ball to one in a space orthogonal to the surface normal.
    This is why we need a householder reflection matrix: it gives us the orthonormal bases that we want.
   </p>
   <p>
    Now let us delve into 4D Lambertian materials.
   </p>
   <p>
    First off, it is very important to understand the 3D Lambertian BRDF.
    I spent some time reading the following two articles: <a href="https://www.oceanopticsbook.info/view/surfaces/lambertian-brdfs">(Ocean Optics)</a>, <a href="https://sakibsaikia.github.io/graphics/2019/09/10/Deriving-Lambertian-BRDF-From-First-Principles.html">(Lambertian BRDF derivation)</a>.
    What I learnt is that in the BRDF, we divide by π (and not 2π as I would have expected) in order to ensure that we have no energy loss when albedo is 1.
    The BRDF is simply constructed in order to ensure that.
    But more importantly than that, I think I gained a better understanding of why the cosine sampling method works for 3D.
   </p>
   <p>
    Because of this, I am able to simply use the equivalent of disk sampling for the 4D case.
    I made the back facing wall a light source and all of the walls Lambertian, and rendered to get this image:
   </p>
   <figure>
    <img src="./render-21.gif" width=50%>
    <figcaption>Figure 21: Pathtracing Lambertian ideal diffusion; 5 samples / pixel</figcaption>
   </figure>
   <hgroup>
    <h3>--- Day 17 ---</h3>
    <span class="right">Aug 01 2025</span>
   </hgroup>
   <p>
    Okay, so let's talk about what sort of image I produced yesterday.
    As expected, with 5 samples, we have a lot of noise, especially since our search space is huge too.
    Additionally, there is a single black pixel in the center of the light source on the back facing wall.
    But... more than that there seems to be a jarring problem.
   </p>
   <p>
    The entirety of the left wall is not illuminated at all.
   </p>
   <p>
    Luckily, I am pretty sure I know why this is the case; it's because of numerical stability with the householder reflection matrix.
    It is important to note that this effect is only hapenning on the wall with a normal pointing in the standard i direction (the direction of <code>e1</code>).
    In this case, we end up dividing by a very small, maybe zero quantity.
   </p>
   <p>
    To avoid this, I have to change the way I calculate the Householder reflection.
    To ensure numerical stability, the ideal method is that if we are too close to <code>+e1</code>, then we instead use <code>-e1</code> and negate the resulting matrix.
   </p>
   <figure>
    <img src="./render-22.gif" width=50%>
    <figcaption>Figure 22: Fixed Householder reflection matrix; 3 samples / pixel</figcaption>
   </figure>
   <p>
    Now, the noise should be significantly reduced when many more samples are taken.
    I will begin work on the
   </p>
   <h2>Volumetric Fog</h2>
   <p>
    In order to add volumetric fog, I have to consider two things: the absorption of light by the fog, and the scattering of the light from the fog.
    Implementing the absorption of light seems very doable since I can simply calculate the absorption based on distance the ray marches.
   </p>
   <h2>Denoising</h2>
   <h2>Next Steps</h2>
   <ul>
    <li>Camera setup via main (and Gram-Schmidt orthonormalization)</li>
    <li></li>
   </ul>
  </main>
  <footer>
   <p>
    Contributing to a lightweight and accessible web.
   </p>
  </footer>
 </body>
</html>
